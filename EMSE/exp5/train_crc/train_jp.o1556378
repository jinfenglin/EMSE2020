Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
ERROR: Unable to locate a modulefile for 'pytorch/1.7.0'
11/10/2020 02:40:21 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
11/10/2020 02:40:21 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-multilingual-cased-config.json from cache at /afs/crc.nd.edu/user/j/jlin6/.cache/torch/transformers/aee7490b1a48646df683dee12f25d9c63ebbf8dce1b7e1a656ce28830d9a7e86.bc76a47cb1c1c2984e48f23afbd3473a944ac1a2be9a8c8200092f5bf62153c9
11/10/2020 02:40:21 - INFO - transformers.configuration_utils -   Model config DistilBertConfig {
  "activation": "gelu",
  "architectures": [
    "DistilBertForMaskedLM"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "output_past": true,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "vocab_size": 119547
}

11/10/2020 02:40:21 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /afs/crc.nd.edu/user/j/jlin6/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729
11/10/2020 02:40:22 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-multilingual-cased-config.json from cache at /afs/crc.nd.edu/user/j/jlin6/.cache/torch/transformers/aee7490b1a48646df683dee12f25d9c63ebbf8dce1b7e1a656ce28830d9a7e86.bc76a47cb1c1c2984e48f23afbd3473a944ac1a2be9a8c8200092f5bf62153c9
11/10/2020 02:40:22 - INFO - transformers.configuration_utils -   Model config DistilBertConfig {
  "activation": "gelu",
  "architectures": [
    "DistilBertForMaskedLM"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "output_past": true,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "vocab_size": 119547
}

11/10/2020 02:40:22 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/distilbert-base-multilingual-cased-pytorch_model.bin from cache at /afs/crc.nd.edu/user/j/jlin6/.cache/torch/transformers/72a6c787412704a6fa6f5d9e5ef7d33c5b80c787e2bbc7d9ad82d7f88fb8f802.89fad86febf14521569023d312560283a922c0884a52f412eef4e96f91513ab2
11/10/2020 02:40:26 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing DistilBertModel.

11/10/2020 02:40:26 - INFO - transformers.modeling_utils -   All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use DistilBertModel for predictions without further training.
11/10/2020 02:40:26 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-multilingual-cased-config.json from cache at /afs/crc.nd.edu/user/j/jlin6/.cache/torch/transformers/aee7490b1a48646df683dee12f25d9c63ebbf8dce1b7e1a656ce28830d9a7e86.bc76a47cb1c1c2984e48f23afbd3473a944ac1a2be9a8c8200092f5bf62153c9
11/10/2020 02:40:26 - INFO - transformers.configuration_utils -   Model config DistilBertConfig {
  "activation": "gelu",
  "architectures": [
    "DistilBertForMaskedLM"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "output_past": true,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "vocab_size": 119547
}

11/10/2020 02:40:26 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /afs/crc.nd.edu/user/j/jlin6/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729
11/10/2020 02:40:26 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-multilingual-cased-config.json from cache at /afs/crc.nd.edu/user/j/jlin6/.cache/torch/transformers/aee7490b1a48646df683dee12f25d9c63ebbf8dce1b7e1a656ce28830d9a7e86.bc76a47cb1c1c2984e48f23afbd3473a944ac1a2be9a8c8200092f5bf62153c9
11/10/2020 02:40:26 - INFO - transformers.configuration_utils -   Model config DistilBertConfig {
  "activation": "gelu",
  "architectures": [
    "DistilBertForMaskedLM"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "output_past": true,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "vocab_size": 119547
}

11/10/2020 02:40:26 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/distilbert-base-multilingual-cased-pytorch_model.bin from cache at /afs/crc.nd.edu/user/j/jlin6/.cache/torch/transformers/72a6c787412704a6fa6f5d9e5ef7d33c5b80c787e2bbc7d9ad82d7f88fb8f802.89fad86febf14521569023d312560283a922c0884a52f412eef4e96f91513ab2
11/10/2020 02:40:30 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing DistilBertModel.

11/10/2020 02:40:30 - INFO - transformers.modeling_utils -   All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use DistilBertModel for predictions without further training.
11/10/2020 02:40:34 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, data_dir='/afs/crc.nd.edu/user/j/jlin6/data/EMSE/Cica', device=device(type='cuda'), exp_name='jp', fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=2, hard_ratio=0.5, learning_rate=1e-05, local_rank=-1, logging_steps=10, max_grad_norm=1.0, max_steps=-1, model_path='/afs/crc.nd.edu/user/j/jlin6/projects/EMSE2020/EMSE/exp3/models/model1', n_gpu=1, neg_sampling='online', no_cuda=False, num_train_epochs=20.0, output_dir='./output', overwrite=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_epoch=5, save_steps=-1, seed=42, tbert_type='siamese', train_num=None, valid_epoch=5, valid_num=200, valid_step=-1, warmup_steps=0, weight_decay=0.0)
11/10/2020 02:40:34 - INFO - EMSE.BERTDataReader -   Creating examples from dataset file at /afs/crc.nd.edu/user/j/jlin6/data/EMSE/Cica/train
update feature:   0%|          | 0/8 [00:00<?, ?it/s]update feature: 100%|██████████| 8/8 [00:00<00:00, 328.97it/s]
update feature:   0%|          | 0/8 [00:00<?, ?it/s]update feature: 100%|██████████| 8/8 [00:00<00:00, 1823.41it/s]
11/10/2020 02:40:34 - INFO - EMSE.BERTDataReader -   Creating examples from dataset file at /afs/crc.nd.edu/user/j/jlin6/data/EMSE/Cica/valid
update feature:   0%|          | 0/9 [00:00<?, ?it/s]update feature: 100%|██████████| 9/9 [00:00<00:00, 224.93it/s]
update feature:   0%|          | 0/9 [00:00<?, ?it/s]update feature: 100%|██████████| 9/9 [00:00<00:00, 816.79it/s]
11/10/2020 02:40:35 - INFO - __main__ -   ***** Running training *****
11/10/2020 02:40:35 - INFO - __main__ -     Num examples = 16
11/10/2020 02:40:35 - INFO - __main__ -     Num Epochs = 20
11/10/2020 02:40:35 - INFO - __main__ -     Instantaneous batch size per GPU = 4
11/10/2020 02:40:35 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 8
11/10/2020 02:40:35 - INFO - __main__ -     Gradient Accumulation steps = 2
11/10/2020 02:40:35 - INFO - __main__ -     Total optimization steps = 40
11/10/2020 02:40:35 - INFO - EMSE.utils -   Loading checkpoint from /afs/crc.nd.edu/user/j/jlin6/projects/EMSE2020/EMSE/exp3/models/model1, remove optimizer and scheduler if you do not want to load them
11/10/2020 02:40:41 - INFO - EMSE.utils -   Loading optimizer...
11/10/2020 02:40:53 - INFO - EMSE.utils -   Loading scheduler...
/afs/crc.nd.edu/user/j/jlin6/.local/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:234: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.
  warnings.warn(SAVE_STATE_WARNING, UserWarning)
11/10/2020 02:40:53 - INFO - __main__ -     Continuing training from checkpoint, will skip to saved global_step
11/10/2020 02:40:53 - INFO - __main__ -     Continuing training from epoch 0, global step 0
Epoch:   0%|          | 0/20 [00:00<?, ?it/s]
Steps:   0%|          | 0/40.0 [00:00<?, ?it/s][A
Steps:   2%|▎         | 1/40.0 [00:00<00:38,  1.01it/s][A
Steps:   5%|▌         | 2/40.0 [00:01<00:33,  1.15it/s][A11/10/2020 02:40:54 - INFO - EMSE.utils -   Saving checkpoint to ./output/jp_11-10_02-40-35/epoch-ckp-0
/afs/crc.nd.edu/user/j/jlin6/.local/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.
  warnings.warn(SAVE_STATE_WARNING, UserWarning)


update embedding:   0%|          | 0/9 [00:00<?, ?it/s][A[A

update embedding:  89%|████████▉ | 8/9 [00:00<00:00, 79.32it/s][A[Aupdate embedding: 100%|██████████| 9/9 [00:00<00:00, 79.21it/s]


update embedding:   0%|          | 0/9 [00:00<?, ?it/s][A[A

update embedding: 100%|██████████| 9/9 [00:00<00:00, 81.73it/s][A[Aupdate embedding: 100%|██████████| 9/9 [00:00<00:00, 81.56it/s]


random_neg_sampling_dataset:   0%|          | 0/9 [00:00<?, ?it/s][A[Arandom_neg_sampling_dataset: 100%|██████████| 9/9 [00:00<00:00, 35478.14it/s]


Classify Evaluating:   0%|          | 0/5 [00:00<?, ?it/s][A[A

Classify Evaluating:  40%|████      | 2/5 [00:00<00:00, 12.12it/s][A[A

Classify Evaluating:  80%|████████  | 4/5 [00:00<00:00, 12.33it/s][A[AClassify Evaluating: 100%|██████████| 5/5 [00:00<00:00, 13.48it/s]
                                             
                                                       [AEpoch:   0%|          | 0/20 [00:20<?, ?it/s]
Steps:   5%|▌         | 2/40.0 [00:20<00:33,  1.15it/s][A

retrival evaluation:   0%|          | 0/21 [00:00<?, ?it/s][A[A

retrival evaluation:  33%|███▎      | 7/21 [00:00<00:00, 69.16it/s][A[A

retrival evaluation:  71%|███████▏  | 15/21 [00:00<00:00, 70.81it/s][A[Aretrival evaluation: 100%|██████████| 21/21 [00:00<00:00, 74.21it/s]
Epoch:   5%|▌         | 1/20 [00:21<06:47, 21.43s/it]
Steps:   8%|▊         | 3/40.0 [00:22<04:09,  6.74s/it][A
Steps:  10%|█         | 4/40.0 [00:22<02:56,  4.90s/it][AEpoch:  10%|█         | 2/20 [00:22<04:36, 15.37s/it]
Steps:  12%|█▎        | 5/40.0 [00:23<02:06,  3.61s/it][A
Steps:  15%|█▌        | 6/40.0 [00:23<01:32,  2.71s/it][AEpoch:  15%|█▌        | 3/20 [00:23<03:09, 11.12s/it]
Steps:  18%|█▊        | 7/40.0 [00:24<01:08,  2.08s/it][A
Steps:  20%|██        | 8/40.0 [00:25<00:52,  1.64s/it][AEpoch:  20%|██        | 4/20 [00:25<02:10,  8.14s/it]
Steps:  22%|██▎       | 9/40.0 [00:25<00:41,  1.32s/it][A
Steps:  25%|██▌       | 10/40.0 [00:26<00:33,  1.11s/it][AEpoch:  25%|██▌       | 5/20 [00:26<01:30,  6.06s/it]
Steps:  28%|██▊       | 11/40.0 [00:26<00:27,  1.04it/s][A
Steps:  30%|███       | 12/40.0 [00:27<00:23,  1.17it/s][A11/10/2020 02:41:20 - INFO - EMSE.utils -   Saving checkpoint to ./output/jp_11-10_02-40-35/epoch-ckp-5


update embedding:   0%|          | 0/9 [00:00<?, ?it/s][A[A

update embedding:  89%|████████▉ | 8/9 [00:00<00:00, 75.51it/s][A[Aupdate embedding: 100%|██████████| 9/9 [00:00<00:00, 75.74it/s]


update embedding:   0%|          | 0/9 [00:00<?, ?it/s][A[A

update embedding: 100%|██████████| 9/9 [00:00<00:00, 84.14it/s][A[Aupdate embedding: 100%|██████████| 9/9 [00:00<00:00, 83.96it/s]


random_neg_sampling_dataset:   0%|          | 0/9 [00:00<?, ?it/s][A[Arandom_neg_sampling_dataset: 100%|██████████| 9/9 [00:00<00:00, 55431.33it/s]


Classify Evaluating:   0%|          | 0/5 [00:00<?, ?it/s][A[A

Classify Evaluating:  40%|████      | 2/5 [00:00<00:00, 12.53it/s][A[A

Classify Evaluating:  80%|████████  | 4/5 [00:00<00:00, 12.64it/s][A[AClassify Evaluating: 100%|██████████| 5/5 [00:00<00:00, 13.76it/s]
                                                     
                                                        [AEpoch:  25%|██▌       | 5/20 [01:09<01:30,  6.06s/it]
Steps:  30%|███       | 12/40.0 [01:09<00:23,  1.17it/s][A

retrival evaluation:   0%|          | 0/21 [00:00<?, ?it/s][A[A

retrival evaluation:  38%|███▊      | 8/21 [00:00<00:00, 74.38it/s][A[A

retrival evaluation:  76%|███████▌  | 16/21 [00:00<00:00, 74.67it/s][A[Aretrival evaluation: 100%|██████████| 21/21 [00:00<00:00, 76.10it/s]
Epoch:  30%|███       | 6/20 [01:09<04:02, 17.31s/it]
Steps:  32%|███▎      | 13/40.0 [01:10<06:03, 13.47s/it][A
Steps:  35%|███▌      | 14/40.0 [01:10<04:09,  9.61s/it][AEpoch:  35%|███▌      | 7/20 [01:10<02:42, 12.47s/it]
Steps:  38%|███▊      | 15/40.0 [01:11<02:52,  6.91s/it][A
Steps:  40%|████      | 16/40.0 [01:12<02:00,  5.02s/it][AEpoch:  40%|████      | 8/20 [01:12<01:49,  9.09s/it]
Steps:  42%|████▎     | 17/40.0 [01:12<01:24,  3.69s/it][A
Steps:  45%|████▌     | 18/40.0 [01:13<01:00,  2.76s/it][AEpoch:  45%|████▌     | 9/20 [01:13<01:13,  6.71s/it]
Steps:  48%|████▊     | 19/40.0 [01:13<00:44,  2.11s/it][A
Steps:  50%|█████     | 20/40.0 [01:14<00:33,  1.66s/it][AEpoch:  50%|█████     | 10/20 [01:14<00:50,  5.06s/it]
Steps:  52%|█████▎    | 21/40.0 [01:15<00:25,  1.34s/it][A
Steps:  55%|█████▌    | 22/40.0 [01:15<00:20,  1.12s/it][A11/10/2020 02:42:09 - INFO - EMSE.utils -   Saving checkpoint to ./output/jp_11-10_02-40-35/epoch-ckp-10


update embedding:   0%|          | 0/9 [00:00<?, ?it/s][A[A

update embedding:  89%|████████▉ | 8/9 [00:00<00:00, 78.15it/s][A[Aupdate embedding: 100%|██████████| 9/9 [00:00<00:00, 77.94it/s]


update embedding:   0%|          | 0/9 [00:00<?, ?it/s][A[A

update embedding:  89%|████████▉ | 8/9 [00:00<00:00, 79.10it/s][A[Aupdate embedding: 100%|██████████| 9/9 [00:00<00:00, 79.24it/s]


random_neg_sampling_dataset:   0%|          | 0/9 [00:00<?, ?it/s][A[Arandom_neg_sampling_dataset: 100%|██████████| 9/9 [00:00<00:00, 47244.98it/s]


Classify Evaluating:   0%|          | 0/5 [00:00<?, ?it/s][A[A

Classify Evaluating:  40%|████      | 2/5 [00:00<00:00, 12.19it/s][A[A

Classify Evaluating:  80%|████████  | 4/5 [00:00<00:00, 12.39it/s][A[AClassify Evaluating: 100%|██████████| 5/5 [00:00<00:00, 13.59it/s]
                                                      
                                                        [AEpoch:  50%|█████     | 10/20 [02:03<00:50,  5.06s/it]
Steps:  55%|█████▌    | 22/40.0 [02:03<00:20,  1.12s/it][A

retrival evaluation:   0%|          | 0/21 [00:00<?, ?it/s][A[A

retrival evaluation:  38%|███▊      | 8/21 [00:00<00:00, 73.00it/s][A[A

retrival evaluation:  76%|███████▌  | 16/21 [00:00<00:00, 73.80it/s][A[Aretrival evaluation: 100%|██████████| 21/21 [00:00<00:00, 76.08it/s]
Epoch:  55%|█████▌    | 11/20 [02:03<02:44, 18.26s/it]
Steps:  57%|█████▊    | 23/40.0 [02:04<04:20, 15.32s/it][A
Steps:  60%|██████    | 24/40.0 [02:04<02:54, 10.90s/it][AEpoch:  60%|██████    | 12/20 [02:04<01:45, 13.14s/it]
Steps:  62%|██████▎   | 25/40.0 [02:05<01:57,  7.81s/it][A
Steps:  65%|██████▌   | 26/40.0 [02:05<01:19,  5.64s/it][AEpoch:  65%|██████▌   | 13/20 [02:05<01:06,  9.55s/it]
Steps:  68%|██████▊   | 27/40.0 [02:06<00:53,  4.13s/it][A
Steps:  70%|███████   | 28/40.0 [02:07<00:36,  3.07s/it][AEpoch:  70%|███████   | 14/20 [02:07<00:42,  7.04s/it]
Steps:  72%|███████▎  | 29/40.0 [02:07<00:25,  2.32s/it][A
Steps:  75%|███████▌  | 30/40.0 [02:08<00:18,  1.80s/it][AEpoch:  75%|███████▌  | 15/20 [02:08<00:26,  5.28s/it]
Steps:  78%|███████▊  | 31/40.0 [02:08<00:12,  1.44s/it][A
Steps:  80%|████████  | 32/40.0 [02:09<00:09,  1.19s/it][A11/10/2020 02:43:02 - INFO - EMSE.utils -   Saving checkpoint to ./output/jp_11-10_02-40-35/epoch-ckp-15


update embedding:   0%|          | 0/9 [00:00<?, ?it/s][A[A

update embedding:  89%|████████▉ | 8/9 [00:00<00:00, 79.97it/s][A[Aupdate embedding: 100%|██████████| 9/9 [00:00<00:00, 79.73it/s]


update embedding:   0%|          | 0/9 [00:00<?, ?it/s][A[A

update embedding: 100%|██████████| 9/9 [00:00<00:00, 81.97it/s][A[Aupdate embedding: 100%|██████████| 9/9 [00:00<00:00, 81.79it/s]


random_neg_sampling_dataset:   0%|          | 0/9 [00:00<?, ?it/s][A[Arandom_neg_sampling_dataset: 100%|██████████| 9/9 [00:00<00:00, 85404.38it/s]


Classify Evaluating:   0%|          | 0/5 [00:00<?, ?it/s][A[A

Classify Evaluating:  40%|████      | 2/5 [00:00<00:00, 12.28it/s][A[A

Classify Evaluating:  80%|████████  | 4/5 [00:00<00:00, 12.43it/s][A[AClassify Evaluating: 100%|██████████| 5/5 [00:00<00:00, 13.59it/s]
                                                      
                                                        [AEpoch:  75%|███████▌  | 15/20 [02:52<00:26,  5.28s/it]
Steps:  80%|████████  | 32/40.0 [02:52<00:09,  1.19s/it][A

retrival evaluation:   0%|          | 0/21 [00:00<?, ?it/s][A[A

retrival evaluation:  38%|███▊      | 8/21 [00:00<00:00, 74.38it/s][A[A

retrival evaluation:  76%|███████▌  | 16/21 [00:00<00:00, 74.92it/s][A[Aretrival evaluation: 100%|██████████| 21/21 [00:00<00:00, 76.72it/s]
Epoch:  80%|████████  | 16/20 [02:52<01:08, 17.00s/it]
Steps:  82%|████████▎ | 33/40.0 [02:53<01:37, 13.96s/it][A
Steps:  85%|████████▌ | 34/40.0 [02:53<00:59,  9.95s/it][AEpoch:  85%|████████▌ | 17/20 [02:53<00:36, 12.26s/it]
Steps:  88%|████████▊ | 35/40.0 [02:54<00:35,  7.14s/it][A
Steps:  90%|█████████ | 36/40.0 [02:55<00:20,  5.18s/it][AEpoch:  90%|█████████ | 18/20 [02:55<00:17,  8.94s/it]
Steps:  92%|█████████▎| 37/40.0 [02:55<00:11,  3.80s/it][A
Steps:  95%|█████████▌| 38/40.0 [02:56<00:05,  2.84s/it][AEpoch:  95%|█████████▌| 19/20 [02:56<00:06,  6.62s/it]
Steps:  98%|█████████▊| 39/40.0 [02:56<00:02,  2.17s/it][A
Steps: 100%|██████████| 40/40.0 [02:57<00:00,  1.70s/it][AEpoch: 100%|██████████| 20/20 [02:57<00:00,  4.99s/it]
                                                        [AEpoch: 100%|██████████| 20/20 [02:57<00:00,  8.87s/it]
11/10/2020 02:43:50 - INFO - EMSE.utils -   Saving checkpoint to ./output/jp_11-10_02-40-35/final_model
Steps: 100%|██████████| 40/40.0 [03:28<00:00,  5.21s/it]
11/10/2020 02:44:21 - INFO - __main__ -   Training finished

evaluate accuracy=0.5


evaluate accuracy=0.5555555555555556


evaluate accuracy=0.5555555555555556


evaluate accuracy=0.5555555555555556

